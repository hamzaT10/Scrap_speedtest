{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Setup Instructions"
      ],
      "metadata": {
        "id": "o4MNSD_6Mh4i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we will set up for running Selenium in Google Colab\n"
      ],
      "metadata": {
        "id": "4pfonhik2krE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LitKpw9t2TEJ",
        "outputId": "bbfb4a2d-81e4-44c3-fbdd-d859db3604be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hit:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:2 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:3 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:4 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Ign:6 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Get:7 https://r2u.stat.illinois.edu/ubuntu jammy Release [5,713 B]\n",
            "Get:8 https://r2u.stat.illinois.edu/ubuntu jammy Release.gpg [793 B]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Hit:10 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:11 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:12 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:13 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,552 kB]\n",
            "Get:14 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,129 kB]\n",
            "Get:15 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,210 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [2,378 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,420 kB]\n",
            "Fetched 16.1 MB in 5s (3,008 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "47 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "\u001b[1;33mW: \u001b[0mSkipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\u001b[0m\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "curl is already the newest version (7.81.0-1ubuntu1.16).\n",
            "unzip is already the newest version (6.0-26ubuntu3.2).\n",
            "wget is already the newest version (1.21.2-2ubuntu1.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 47 not upgraded.\n",
            "--2024-07-27 19:20:52--  http://archive.ubuntu.com/ubuntu/pool/main/libu/libu2f-host/libu2f-udev_1.1.4-1_all.deb\n",
            "Resolving archive.ubuntu.com (archive.ubuntu.com)... 91.189.91.82, 185.125.190.81, 91.189.91.81, ...\n",
            "Connecting to archive.ubuntu.com (archive.ubuntu.com)|91.189.91.82|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3708 (3.6K) [application/vnd.debian.binary-package]\n",
            "Saving to: ‘libu2f-udev_1.1.4-1_all.deb’\n",
            "\n",
            "libu2f-udev_1.1.4-1 100%[===================>]   3.62K  --.-KB/s    in 0s      \n",
            "\n",
            "2024-07-27 19:20:52 (73.7 MB/s) - ‘libu2f-udev_1.1.4-1_all.deb’ saved [3708/3708]\n",
            "\n",
            "Selecting previously unselected package libu2f-udev.\n",
            "(Reading database ... 123589 files and directories currently installed.)\n",
            "Preparing to unpack libu2f-udev_1.1.4-1_all.deb ...\n",
            "Unpacking libu2f-udev (1.1.4-1) ...\n",
            "Setting up libu2f-udev (1.1.4-1) ...\n",
            "--2024-07-27 19:20:53--  https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb\n",
            "Resolving dl.google.com (dl.google.com)... 172.217.0.78, 2607:f8b0:4025:803::200e\n",
            "Connecting to dl.google.com (dl.google.com)|172.217.0.78|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 109166380 (104M) [application/x-debian-package]\n",
            "Saving to: ‘google-chrome-stable_current_amd64.deb’\n",
            "\n",
            "google-chrome-stabl 100%[===================>] 104.11M  90.8MB/s    in 1.1s    \n",
            "\n",
            "2024-07-27 19:20:54 (90.8 MB/s) - ‘google-chrome-stable_current_amd64.deb’ saved [109166380/109166380]\n",
            "\n",
            "Selecting previously unselected package google-chrome-stable.\n",
            "(Reading database ... 123593 files and directories currently installed.)\n",
            "Preparing to unpack google-chrome-stable_current_amd64.deb ...\n",
            "Unpacking google-chrome-stable (127.0.6533.72-1) ...\n",
            "\u001b[1mdpkg:\u001b[0m dependency problems prevent configuration of google-chrome-stable:\n",
            " google-chrome-stable depends on libvulkan1; however:\n",
            "  Package libvulkan1 is not installed.\n",
            "\n",
            "\u001b[1mdpkg:\u001b[0m error processing package google-chrome-stable (--install):\n",
            " dependency problems - leaving unconfigured\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Errors were encountered while processing:\n",
            " google-chrome-stable\n",
            "--2024-07-27 19:21:09--  https://chromedriver.storage.googleapis.com/114.0.5735.90/chromedriver_linux64.zip\n",
            "Resolving chromedriver.storage.googleapis.com (chromedriver.storage.googleapis.com)... 142.250.65.123, 172.217.15.251, 172.217.164.27, ...\n",
            "Connecting to chromedriver.storage.googleapis.com (chromedriver.storage.googleapis.com)|142.250.65.123|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7407250 (7.1M) [application/zip]\n",
            "Saving to: ‘/tmp/chromedriver_linux64.zip’\n",
            "\n",
            "chromedriver_linux6 100%[===================>]   7.06M  --.-KB/s    in 0.04s   \n",
            "\n",
            "2024-07-27 19:21:09 (163 MB/s) - ‘/tmp/chromedriver_linux64.zip’ saved [7407250/7407250]\n",
            "\n",
            "Archive:  /tmp/chromedriver_linux64.zip\n",
            "  inflating: /tmp/chromedriver       \n",
            "  inflating: /tmp/LICENSE.chromedriver  \n",
            "Collecting selenium\n",
            "  Downloading selenium-4.23.1-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: urllib3<3,>=1.26 in /usr/local/lib/python3.10/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (2.0.7)\n",
            "Collecting trio~=0.17 (from selenium)\n",
            "  Downloading trio-0.26.0-py3-none-any.whl.metadata (8.8 kB)\n",
            "Collecting trio-websocket~=0.9 (from selenium)\n",
            "  Downloading trio_websocket-0.11.1-py3-none-any.whl.metadata (4.7 kB)\n",
            "Requirement already satisfied: certifi>=2021.10.8 in /usr/local/lib/python3.10/dist-packages (from selenium) (2024.7.4)\n",
            "Requirement already satisfied: typing_extensions~=4.9 in /usr/local/lib/python3.10/dist-packages (from selenium) (4.12.2)\n",
            "Requirement already satisfied: websocket-client~=1.8 in /usr/local/lib/python3.10/dist-packages (from selenium) (1.8.0)\n",
            "Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (23.2.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (2.4.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (3.7)\n",
            "Collecting outcome (from trio~=0.17->selenium)\n",
            "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.2.2)\n",
            "Collecting wsproto>=0.14 (from trio-websocket~=0.9->selenium)\n",
            "  Downloading wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
            "Collecting h11<1,>=0.9.0 (from wsproto>=0.14->trio-websocket~=0.9->selenium)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Downloading selenium-4.23.1-py3-none-any.whl (9.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.4/9.4 MB\u001b[0m \u001b[31m37.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trio-0.26.0-py3-none-any.whl (475 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m475.7/475.7 kB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trio_websocket-0.11.1-py3-none-any.whl (17 kB)\n",
            "Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
            "Downloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
            "Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: outcome, h11, wsproto, trio, trio-websocket, selenium\n",
            "Successfully installed h11-0.14.0 outcome-1.3.0.post0 selenium-4.23.1 trio-0.26.0 trio-websocket-0.11.1 wsproto-1.2.0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "# Set up for running selenium in Google Colab\n",
        "## You don't need to run this code if you do it in Jupyter notebook, or other local Python setting\n",
        "%%shell\n",
        "sudo apt -y update\n",
        "sudo apt install -y wget curl unzip\n",
        "wget http://archive.ubuntu.com/ubuntu/pool/main/libu/libu2f-host/libu2f-udev_1.1.4-1_all.deb\n",
        "dpkg -i libu2f-udev_1.1.4-1_all.deb\n",
        "wget https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb\n",
        "dpkg -i google-chrome-stable_current_amd64.deb\n",
        "CHROME_DRIVER_VERSION=`curl -sS chromedriver.storage.googleapis.com/LATEST_RELEASE`\n",
        "wget -N https://chromedriver.storage.googleapis.com/$CHROME_DRIVER_VERSION/chromedriver_linux64.zip -P /tmp/\n",
        "unzip -o /tmp/chromedriver_linux64.zip -d /tmp/\n",
        "chmod +x /tmp/chromedriver\n",
        "mv /tmp/chromedriver /usr/local/bin/chromedriver\n",
        "pip install selenium\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install chromedriver-autoinstaller   \n"
      ],
      "metadata": {
        "id": "UeiNnTHf22Jx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install chromedriver-autoinstaller"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sOQbpkza2x8e",
        "outputId": "d9c9c79b-354c-41ac-8ed2-641b0f962f3a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting chromedriver-autoinstaller\n",
            "  Downloading chromedriver_autoinstaller-0.6.4-py3-none-any.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: packaging>=23.1 in /usr/local/lib/python3.10/dist-packages (from chromedriver-autoinstaller) (24.1)\n",
            "Downloading chromedriver_autoinstaller-0.6.4-py3-none-any.whl (7.6 kB)\n",
            "Installing collected packages: chromedriver-autoinstaller\n",
            "Successfully installed chromedriver-autoinstaller-0.6.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set up Selenium WebDriver with Chromedriver Auto-installation"
      ],
      "metadata": {
        "id": "AyUW6XxG279P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary modules from Selenium\n",
        "from selenium import webdriver\n",
        "import chromedriver_autoinstaller\n",
        "def initialize_driver():\n",
        "    # Configure Chrome options\n",
        "    chrome_options = webdriver.ChromeOptions()\n",
        "    chrome_options.add_argument('--headless')  # Ensure GUI is off\n",
        "    chrome_options.add_argument('--no-sandbox')\n",
        "    chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "    chrome_options.add_argument('--disable-blink-features=AutomationControlled')\n",
        "    chrome_options.add_experimental_option('excludeSwitches', ['enable-logging'])\n",
        "\n",
        "    # Use chromedriver_autoinstaller to install the latest version of ChromeDriver\n",
        "    chromedriver_autoinstaller.install()\n",
        "\n",
        "    # Set up the WebDriver with configured options\n",
        "    driver = webdriver.Chrome(options=chrome_options)\n",
        "\n",
        "    return driver"
      ],
      "metadata": {
        "id": "Wp-wSOiX2977"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Necessary Libraries"
      ],
      "metadata": {
        "id": "zdFk15WT3BTL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary modules from Selenium\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "\n",
        "# Import BeautifulSoup for HTML parsing\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Import time for adding delays\n",
        "import time\n",
        "\n",
        "# Import csv for handling CSV files\n",
        "import csv\n"
      ],
      "metadata": {
        "id": "Jlyw2xbg3C_N"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Function to Scrape Main Page: Extract Links from Table"
      ],
      "metadata": {
        "id": "1Dm-qWW43GPn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def scrape_main_page(url, driver, table_xpath, data_list_name):\n",
        "    \"\"\"\n",
        "    Scrape data from the given URL and table XPath and store the results in a list.\n",
        "\n",
        "    :param url: The URL of the page to scrape.\n",
        "    :param driver: The Selenium WebDriver instance.\n",
        "    :param table_xpath: The XPath of the table to scrape.\n",
        "    :param data_list_name: The name of the list to store extracted links.\n",
        "    :return: A list of extracted links.\n",
        "    \"\"\"\n",
        "    # Make an HTTP request to the main page\n",
        "    driver.get(url)\n",
        "\n",
        "    # Use WebDriverWait to wait for the \"Show All\" button to be clickable\n",
        "    try:\n",
        "        button = WebDriverWait(driver, 10).until(\n",
        "            EC.element_to_be_clickable((By.XPATH, '/html/body/div[5]/div/div[2]/div[2]/div/div/div/div[3]/button'))\n",
        "        )\n",
        "        driver.execute_script(\"arguments[0].click();\", button)\n",
        "    except Exception as e:\n",
        "        print(\"Error clicking the search button:\", e)\n",
        "        return []\n",
        "\n",
        "    # Use WebDriverWait to wait for the table to be present\n",
        "    try:\n",
        "        table = WebDriverWait(driver, 10).until(\n",
        "            EC.presence_of_element_located((By.XPATH, table_xpath))\n",
        "        )\n",
        "    except Exception as e:\n",
        "        print(\"Error finding the table:\", e)\n",
        "        return []\n",
        "\n",
        "    # Pause briefly to allow table data to load, especially if AJAX is involved\n",
        "    time.sleep(2)\n",
        "\n",
        "    data_list = []  # List to store extracted links\n",
        "\n",
        "    # Extract the table rows using Selenium\n",
        "    rows = table.find_elements(By.XPATH, './tbody/tr[@class=\"data-result results\"]')\n",
        "\n",
        "    if not rows:\n",
        "        print(\"No rows found in the table. Check the HTML structure.\")\n",
        "    for row in rows:\n",
        "        try:\n",
        "            # Use Selenium to find the column and link\n",
        "            country_td = row.find_element(By.CLASS_NAME, 'country')\n",
        "            link_tag = country_td.find_element(By.TAG_NAME, 'a')\n",
        "            link = link_tag.get_attribute('href')\n",
        "            data_list.append(link)\n",
        "        except Exception as e:\n",
        "            print(f\"Error extracting link from row: {e}\")\n",
        "\n",
        "    return data_list"
      ],
      "metadata": {
        "id": "-4UXmamd84-A"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Function to Scrape connections details"
      ],
      "metadata": {
        "id": "jCvpH4wIA0dw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def scrape_country_details(country_url, driver, rank, data_type):\n",
        "    \"\"\"\n",
        "    Scrapes country details from the provided URL based on the data type (Broadband or Mobile).\n",
        "\n",
        "    Parameters:\n",
        "    - country_url: URL of the country details page.\n",
        "    - driver: Selenium WebDriver instance.\n",
        "    - rank: Rank of the country.\n",
        "    - data_type: Type of data to scrape ('Broadband' or 'Mobile').\n",
        "\n",
        "    Returns:\n",
        "    - A dictionary with the country details.\n",
        "    \"\"\"\n",
        "    # Make an HTTP request to the country details page\n",
        "    driver.get(country_url)\n",
        "    # Add a delay to ensure the page is fully loaded\n",
        "    time.sleep(2)\n",
        "    # Parse the HTML content with BeautifulSoup\n",
        "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
        "\n",
        "    # Extract the country name from the page header\n",
        "    page_header = soup.select_one('div.page-header-title')\n",
        "    if page_header:\n",
        "        full_text = page_header.get_text(separator=' ', strip=True)\n",
        "        country_name = ' '.join(full_text.split()[:-4]).strip()\n",
        "    else:\n",
        "        country_name = 'Unknown'\n",
        "\n",
        "    # Define selectors and parent container based on the data type\n",
        "    if data_type == 'Broadband':\n",
        "        container_id = 'column-fixedMean'\n",
        "    elif data_type == 'Mobile':\n",
        "        container_id = 'column-mobileMean'\n",
        "    else:\n",
        "        raise ValueError(\"Invalid data_type. Must be 'Broadband' or 'Mobile'.\")\n",
        "\n",
        "    # Locate the correct container\n",
        "    container = soup.select_one(f'div#{container_id} .headings')\n",
        "\n",
        "    # Extract data from the selected container\n",
        "    if container:\n",
        "        # Extract the download speed\n",
        "        download_speed = container.select_one('div.result-group.result-group-icon.download span.number')\n",
        "        download_speed = download_speed.text.strip() if download_speed else 'Unknown'\n",
        "\n",
        "        # Extract the upload speed\n",
        "        upload_speed = container.select_one('div.result-group.result-group-icon.upload span.number')\n",
        "        upload_speed = upload_speed.text.strip() if upload_speed else 'Unknown'\n",
        "\n",
        "        # Extract the latency\n",
        "        latency = container.select_one('div.result-group.result-group-icon.latency span.number')\n",
        "        latency = latency.text.strip() if latency else 'Unknown'\n",
        "    else:\n",
        "        download_speed = upload_speed = latency = 'Unknown'\n",
        "\n",
        "    return {\n",
        "        'rank': rank,\n",
        "        'country': country_name,\n",
        "        'upload_speed': upload_speed,\n",
        "        'download_speed': download_speed,\n",
        "        'latency': latency\n",
        "    }\n"
      ],
      "metadata": {
        "id": "GkzXkbx8o5Hv"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Function to save data to a CSV File"
      ],
      "metadata": {
        "id": "cOe_Q8HKA63q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def save_to_csv(data, file_name='data1.csv'):\n",
        "    if not data:\n",
        "        print(\"No data to save.\")\n",
        "        return\n",
        "\n",
        "    # Assuming all dictionaries have the same keys\n",
        "    headers = data[0].keys()\n",
        "\n",
        "    # Open the CSV file in write mode, create a new file if it doesn't exist\n",
        "    with open(file_name, 'w', newline='', encoding='utf-8') as csvfile:\n",
        "        # Create a CSV writer with specified fieldnames\n",
        "        writer = csv.DictWriter(csvfile, fieldnames=headers)\n",
        "\n",
        "        # Write the headers\n",
        "        writer.writeheader()\n",
        "\n",
        "        # Write the data to the CSV file\n",
        "        writer.writerows(data)\n",
        "\n",
        "    print(f\"Data saved to {file_name}.\")\n"
      ],
      "metadata": {
        "id": "qevjjbPGNXg4"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main function to execute the scraping process.\n"
      ],
      "metadata": {
        "id": "FUhpHCo8BLAm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "\n",
        "  # Initialize the Selenium WebDriver\n",
        "  driver = initialize_driver()\n",
        "\n",
        "  # URL of the page to scrape\n",
        "  main_url = 'https://www.speedtest.net/global-index'\n",
        "\n",
        "   # Scrape Fixed Broadband data\n",
        "  data_Broadband = scrape_main_page(main_url, driver, '//*[@id=\"column-fixedMedian\"]/div[1]/div/table', 'data_Broadband')\n",
        "  # Scrape Mobile data\n",
        "  data_Mobile = scrape_main_page(main_url, driver, '//*[@id=\"column-mobileMedian\"]/div[1]/div/table', 'data_Mobile')\n",
        "\n",
        "  # Scrape country details for Broadband and save to CSV\n",
        "  broadband_data = [scrape_country_details(link, driver, rank,'Broadband') for rank, link in enumerate(data_Broadband, start=1)]\n",
        "  save_to_csv(broadband_data, 'data_broadband.csv')\n",
        "\n",
        "  # Scrape country details for Mobile and save to CSV\n",
        "  mobile_data = [scrape_country_details(link, driver, rank,'Mobile') for rank, link in enumerate(data_Mobile, start=1)]\n",
        "  save_to_csv(mobile_data, 'data_mobile.csv')\n",
        "\n",
        "  # Quit the WebDriver session\n",
        "  driver.quit()\n",
        "\n",
        "  print(\"Data scraping process completed successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i_ADRjyn--cV",
        "outputId": "15e41f47-d05e-40a6-cfe5-0fc8267dc2b9"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data saved to data_broadband.csv.\n",
            "Data saved to data_mobile.csv.\n",
            "Data scraping process completed successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Converting csv to sql file"
      ],
      "metadata": {
        "id": "HT6VPHDkBTid"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pymysql"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WdBINJ3q8smo",
        "outputId": "35fd9d5b-f95e-4b2c-a8ed-298d11aacb07"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pymysql\n",
            "  Downloading PyMySQL-1.1.1-py3-none-any.whl.metadata (4.4 kB)\n",
            "Downloading PyMySQL-1.1.1-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.0/45.0 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pymysql\n",
            "Successfully installed pymysql-1.1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import pymysql\n",
        "\n",
        "def escape_special_characters(value):\n",
        "    \"\"\"Escape special characters in a string for SQL insertion.\"\"\"\n",
        "    if isinstance(value, str):\n",
        "        # Use pymysql's built-in method to escape string safely\n",
        "        return pymysql.converters.escape_string(value)\n",
        "    return value\n",
        "\n",
        "def csv_to_sql(csv_file, table_name, sql_file):\n",
        "    # Read the CSV file\n",
        "    df = pd.read_csv(csv_file)\n",
        "\n",
        "    # Open the SQL file with UTF-8 encoding\n",
        "    with open(sql_file, 'w', encoding='utf-8') as f:\n",
        "        # Write SQL insert statements\n",
        "        for i, row in df.iterrows():\n",
        "            try:\n",
        "                # Escape special characters in the country field\n",
        "                country = escape_special_characters(row['country'])\n",
        "                rank = int(row['rank'])\n",
        "                download_speed = float(row['download_speed']) if pd.notna(row['download_speed']) else None\n",
        "                upload_speed = float(row['upload_speed']) if pd.notna(row['upload_speed']) else None\n",
        "                latency = float(row['latency']) if pd.notna(row['latency']) else None\n",
        "\n",
        "                # Construct the SQL insert statement\n",
        "                sql = f\"INSERT INTO {table_name} (country, rank, download, upload, latency) VALUES (\"\n",
        "                sql += f\"'{country}', {rank}, {download_speed}, {upload_speed}, {latency});\\n\"\n",
        "\n",
        "                # Write the SQL statement to the file\n",
        "                f.write(sql)\n",
        "\n",
        "            except ValueError as e:\n",
        "                print(f\"Error processing row {i}: {e}\")\n",
        "            except KeyError as e:\n",
        "                print(f\"Missing column in CSV file: {e}\")\n",
        "\n",
        "# Convert CSV files to SQL\n",
        "csv_to_sql('data_mobile.csv', 'mobile', 'mobile_data.sql')\n",
        "csv_to_sql('data_broadband.csv', 'broadband', 'broadband_data.sql')\n"
      ],
      "metadata": {
        "id": "c6d9xQkJzREb"
      },
      "execution_count": 15,
      "outputs": []
    }
  ]
}